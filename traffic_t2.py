# -*- coding: utf-8 -*-
"""Traffic_T2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O7Dv44sGBQAe45MEb_vYGD4CZnR-xw63
"""

!pip install ultralytics
import ultralytics
ultralytics.checks()
from ultralytics import YOLO

from google.colab import drive
drive.mount('/content/drive')

df= '/content/drive/MyDrive/Traffic.S/Vehicle_Detection_Image_Dataset/data.yaml'

model = YOLO("yolov9e.yaml")
model = YOLO("yolov9e.pt")
model = YOLO("yolov9e.yaml").load("yolov9e.pt")

results = model.train(data=df, epochs=1, imgsz=640)



from ultralytics import YOLO

# Load your trained YOLOv9e model
model = YOLO("/content/runs/detect/train/weights/best.pt")
print("✅ YOLOv9e model loaded successfully!")

from ultralytics import YOLO

# Load your trained YOLOv9e model
model = YOLO("/content/yolov9e.pt")
print("✅ YOLOv9e model loaded successfully!")

# Path to sample image
image_path = "/content/drive/MyDrive/Traffic.S/Vehicle_Detection_Image_Dataset/sample_image 06.jpg"

# Predict
results = model.predict(source=image_path, conf=0.4)

# Show the detection
results[0].show()

# Optional: Save annotated image
results[0].save(filename="/content/detected_sample.jpg")

# Path to sample image
image_path = "/content/drive/MyDrive/Traffic.S/Vehicle_Detection_Image_Dataset/sample_image 07.jpg"

# Predict
results = model.predict(source=image_path, conf=0.4)

# Show the detection
results[0].show()

# Optional: Save annotated image
results[0].save(filename="/content/detected_sample.jpg")

# Path to sample image
image_path = "/content/drive/MyDrive/Traffic.S/Vehicle_Detection_Image_Dataset/sample_image 08.jpg"

# Predict
results = model.predict(source=image_path, conf=0.4)

# Show the detection
results[0].show()

# Optional: Save annotated image
results[0].save(filename="/content/detected_sample.jpg")

from ultralytics import YOLO
import cv2
from IPython.display import Video
import os

# 1️⃣ Load model
model_path = "/content/yolov9e.pt"  # your trained or pretrained weights
model = YOLO(model_path)

# 2️⃣ Input and output paths
video_path = "/content/drive/MyDrive/Traffic.S/Vehicle_Detection_Image_Dataset/sample_video 02.mp4"
output_path = "/content/drive/MyDrive/Traffic.S/detectionsvideo/vehicle_flow_output.mp4"

os.makedirs(os.path.dirname(output_path), exist_ok=True)

# 3️⃣ Run detection on the video and save
results = model.predict(
    source=video_path,
    save=True,
    save_txt=True,
    save_conf=True,
    project=os.path.dirname(output_path),
    name="vehicle_flow"
)

# The predicted video will be saved inside:
# /content/drive/MyDrive/Traffic.S/detectionsvideo/vehicle_flow

# 4️⃣ Find the saved video path
predicted_video_path = os.path.join(os.path.dirname(output_path), "vehicle_flow", os.path.basename(video_path))

# 5️⃣ Display video inside Colab
Video(predicted_video_path, embed=True, width=960)

import shutil
import os

# Source path where YOLO saved the video
src_path = "/content/drive/MyDrive/Traffic.S/Vehicle_Detection_Image_Dataset/sample_video.mp4"

# Destination path where you want to save the video
dst_path = "/content/drive/MyDrive/Traffic.S/final_vehicle_detection.mp4"

# Copy the video file
shutil.copyfile(src_path, dst_path)

print(f"✅ Video copied and saved to:\n{dst_path}")

from ultralytics import YOLO
import cv2
import os

# 1️⃣ Load YOLO model
model_path = "/content/yolov9e.pt"  # your trained/pretrained weights
model = YOLO(model_path)

# 2️⃣ Paths
video_path = "/content/drive/MyDrive/Traffic.S/Vehicle_Detection_Image_Dataset/sample_video 02.mp4"
output_path = "/content/drive/MyDrive/Traffic.S/detectionsvideo/vehicle_flow_counted.mp4"

os.makedirs(os.path.dirname(output_path), exist_ok=True)

# 3️⃣ Open video
cap = cv2.VideoCapture(video_path)
fps = cap.get(cv2.CAP_PROP_FPS)
width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

fourcc = cv2.VideoWriter_fourcc(*"mp4v")
out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))

# 4️⃣ Define class IDs for car/bus/truck/motorbike/person
# YOLOv9/COCO class IDs
target_classes = {
    0: "person",     # ID 0
    2: "car",        # ID 2
    3: "motorbike",  # ID 3
    5: "bus",        # ID 5
    7: "truck"       # ID 7
}

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Run detection
    results = model.predict(frame, conf=0.3, verbose=False)

    # Count objects
    counts = {name: 0 for name in target_classes.values()}
    for box in results[0].boxes:
        cls_id = int(box.cls[0])
        if cls_id in target_classes:
            counts[target_classes[cls_id]] += 1
            # Draw bounding boxes
            xyxy = box.xyxy[0].cpu().numpy().astype(int)
            label = f"{target_classes[cls_id]} {box.conf[0]:.2f}"
            cv2.rectangle(frame, (xyxy[0], xyxy[1]), (xyxy[2], xyxy[3]), (0, 255, 0), 2)
            cv2.putText(frame, label, (xyxy[0], xyxy[1] - 5),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Draw counts in top-left corner
    y_offset = 30
    for obj, count in counts.items():
        text = f"{obj}: {count}"
        cv2.putText(frame, text, (10, y_offset),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        y_offset += 25

    # Write frame to output
    out.write(frame)

cap.release()
out.release()
cv2.destroyAllWindows()

print(f"✅ Processed video saved at: {output_path}")